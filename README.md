
# â™Ÿï¸ AlphaZero Playground

Play against real AlphaZero agents in **Tic Tac Toe** and **Connect Four**, trained completely from scratch using **MCTS + ResNet** and **self-play reinforcement learning**.

âœ… Web-based UI  
ğŸ§  PyTorch AlphaZero backend  
â˜ï¸ Deployed on Render

---

## ğŸŒ Live Demo

- **Frontend**: [https://alphazero-playground-frontend.onrender.com](https://alphazero-playground-frontend.onrender.com)  
- **Backend API**: [https://alphazero-playground-backend.onrender.com](https://alphazero-playground-backend.onrender.com)

> âš ï¸ May take ~10s on first load due to cold start and MCTS on CPU.

---

## ğŸ•¹ï¸ Features

- âœ… Play against AlphaZero in Tic Tac Toe & Connect Four  
- ğŸ¯ View predicted move probabilities from MCTS  
- ğŸ§  Real self-play agents â€” no hardcoded logic or rules  
- ğŸ› ï¸ Modular codebase (games, network, MCTS, backend separated)

---

## ğŸ§  How It Works

This project implements a simplified version of **AlphaZero**:

- **ResNet**: Custom residual network for policy + value prediction  
- **MCTS**: Monte Carlo Tree Search guided by the network  
- **Self-play training**: No human data, only reinforcement learning  
- **Inference**: Each move runs MCTS with multiple simulations

---

## ğŸ“ Project Structure

alphazero-playground/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py              # FastAPI app
â”‚   â””â”€â”€ model/
â”‚       â”œâ”€â”€ game.py          # Game logic (Tic Tac Toe, Connect Four)
â”‚       â”œâ”€â”€ network.py       # ResNet definition
â”‚       â”œâ”€â”€ alpha\_zero.py    # MCTS logic
â”‚       â”œâ”€â”€ model\_tic\_tac\_toe.pt
â”‚       â””â”€â”€ model\_connect4.pt
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html           # Landing page
â”‚   â”œâ”€â”€ tictactoe.html       # Game UI
â”‚   â”œâ”€â”€ connect4.html
â”‚   â”œâ”€â”€ tictactoe.js         # Game logic (AJAX â†’ FastAPI)
â”‚   â”œâ”€â”€ connect4.js
â”‚   â””â”€â”€ styles/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


---

## ğŸ› ï¸ Local Setup

### âœ… Prerequisites

- Python 3.9+
- Node.js (optional, only for advanced static serving)

---

### ğŸ“¦ Backend (FastAPI)

# Clone the repo
git clone https://github.com/yourusername/alphazero-playground.git
cd alphazero-playground/backend

# Install dependencies
pip install -r requirements.txt

# Run the server
uvicorn main:app --reload


> Visit: [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs) to test endpoints

---

### ğŸŒ Frontend

bash
cd ../frontend
open index.html


Or run a simple static server:

# With Python 3
python -m http.server 8001

> Make sure to update the JS files with correct backend URL if testing locally.

---

## ğŸš€ Deployment on Render

### ğŸ§  Backend (FastAPI)

1. Create a **Web Service** on [Render](https://render.com/)
2. Root directory: `backend`
3. Start Command:

   uvicorn main:app --host 0.0.0.0 --port 10000
   
4. Add `model_tic_tac_toe.pt` and `model_connect4.pt` to `backend/model/`
5. Add any necessary environment settings (e.g. `PORT=10000`)

---

### ğŸŒ Frontend (Static Site)

1. Create a **Static Site** on Render
2. Root directory: `frontend`
3. Build command: *(leave blank)*
4. Publish directory: `frontend`
5. In `tictactoe.js` and `connect4.js`, change API URLs to match backend Render URL.

---

## âš ï¸ Performance Notes

This is a **real AlphaZero** implementation using full MCTS:

* Each move uses 4â€“8 tree search simulations
* Every simulation queries a ResNet for policy + value
* Hosted on free-tier CPU â†’ \~2â€“6s per move

### âš¡ Optimization Ideas

* TorchScript / ONNX export for model inference
* Reduce number of MCTS searches for faster demo
* Async queue + background worker
* Replace MCTS with policy-only inference for quick response mode

---




## ğŸ™‹ FAQ

**Q: Why is it slow?**
A: Each move runs real MCTS with a deep neural network on CPU. This is not a heuristic bot â€” it's actual AlphaZero inference.

**Q: Can it run faster?**
A: Yes â€” quantization, GPU, or switching to policy-only inference would help.

**Q: Is this really AlphaZero?**
A: It follows the core principles: self-play, MCTS-guided search, deep network with policy & value heads, and no prior knowledge.

---

## ğŸ¤ Contributing

Pull requests welcome! You can help with:

* UI/UX improvements
* Add more games (Chess, Gomoku, etc.)
* TorchScript/ONNX export
* Add backend job queue for async inference

---

## ğŸ§¾ License

MIT License â€” use freely for learning, modification, and extension.

---

## ğŸ‘¤ Author

**\Abde Abitalib Merchant**
MS in Computer Science @ USC
Passionate about AI, education, and building cool stuff.

---

## ğŸŒ Letâ€™s Connect

* GitHub: [github.com/yourusername](https://github.com/yourusername)
* LinkedIn: [linkedin.com/in/yourusername](https://linkedin.com/in/yourusername)


